{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from openpyxl import Workbook\n",
    "from openpyxl import load_workbook\n",
    "import re\n",
    "from itertools import chain\n",
    "from sklearn.utils import shuffle\n",
    "import nltk  \n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_romanian_data():\n",
    "    wb = load_workbook('../articole.xlsx')\n",
    "    sheet = wb['Foaie1']\n",
    "    data_pos=[]\n",
    "    data_neg=[]\n",
    "    data_doubt=[]\n",
    "    value = ''\n",
    "    for i in range(1, len(sheet['B'])):\n",
    "        value = str(sheet['B' + str(i)].value).replace('â€¢', '') # remove special characters\n",
    "        if bool(re.match('^(?=.*[a-zA-Z])', str(sheet['B' + str(i)].value))): # check if string not empty and has letters\n",
    "            if sheet['A' + str(i)].value == 1:\n",
    "                data_pos.append(value)\n",
    "            elif sheet['A' + str(i)].value == 0:\n",
    "                data_doubt.append(value)\n",
    "            elif not sheet['A' + str(i)].value:\n",
    "                data_neg.append(value)\n",
    "    print(\"data pos len: \" + str(len(data_pos)))\n",
    "    print( \"data neg len: \" + str(len(data_neg)))\n",
    "    \n",
    "#     low_limit = 3000\n",
    "#     high_limit = 10000\n",
    "#     test_pos = data_pos[:low_limit]\n",
    "#     train_pos = data_pos[low_limit:high_limit]\n",
    "#     test_neg = data_neg[:low_limit]\n",
    "    limit = 2400\n",
    "    test_pos = data_pos[:limit]\n",
    "    train_pos = data_pos[limit:]\n",
    "    test_neg = data_neg[:limit]\n",
    "    train_neg = data_neg[limit:len(data_pos)] # we make sure len of neg data = len of pos data(len neg > len pos initially)\n",
    "    train_pos = [str(w).lower() for w in train_pos]\n",
    "    train_pos = ([\" \".join(j for j in w.split() if len(j) >= 2) for w in train_pos])\n",
    "\n",
    "    train_neg = [str(w).lower() for w in train_neg]\n",
    "    train_neg = ([\" \".join(j for j in w.split() if len(j) >= 2) for w in train_neg])\n",
    "\n",
    "    test_pos = [str(w).lower() for w in test_pos]\n",
    "\n",
    "    test_neg = [str(w).lower() for w in test_neg]\n",
    "    return train_pos, train_neg, test_pos, test_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_and_test(train_pos, train_neg, test_pos, test_neg):\n",
    "    X_train = list(chain(train_pos, train_neg))\n",
    "    y_train = np.concatenate((np.ones(len(train_pos), int), np.zeros(len(train_neg), int)))\n",
    "\n",
    "    X_test = list(chain(test_pos, test_neg))\n",
    "    y_test = np.concatenate((np.ones(len(test_pos), int), np.zeros(len(test_neg), int)))\n",
    "\n",
    "    X_train_shuffled, y_train_shuffled =  shuffle(X_train, y_train)\n",
    "    X_test_shuffled, y_test_shuffled = shuffle(X_test, y_test)\n",
    "\n",
    "    return X_train_shuffled, y_train_shuffled, X_test_shuffled, y_test_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(sentences_list, updated_stopwords):\n",
    "    filtered_sentence = []\n",
    "    for sentence in sentences_list:\n",
    "        filtered_sentence.append([w for w in sentence if not w in updated_stopwords])\n",
    "    return repair_sentence(filtered_sentence)\n",
    "\n",
    "\n",
    "def remove_punctuation(from_train_data):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    no_punctuation = [w.translate(table) for w in from_train_data]\n",
    "    numbers = re.compile('[0-9]')\n",
    "    plain_text = [numbers.sub(\"\", word) for word in no_punctuation]\n",
    "    return plain_text\n",
    "    \n",
    "\n",
    "def remove_spaces(from_train_data):\n",
    "    clean_spaces_data = []\n",
    "    for sentence in from_train_data:\n",
    "        clean_spaces_data.append(re.sub(' +', ' ', sentence).rstrip().lstrip())\n",
    "    return clean_spaces_data\n",
    "\n",
    "def repair_sentence(sentence_list):\n",
    "    return [' '.join(map(str, element)) for element in sentence_list]\n",
    "\n",
    "\n",
    "def update_stopwords(stopwords):\n",
    "    do_no_remove_these_sw = ['not', 'no', 'can','has','have','had','must','shan','do', 'should','was','were','won',\n",
    "                             'are','cannot','does','ain', 'could', 'did', 'is', 'might', 'need', 'would']\n",
    "    return [word for word in stopwords if word not in do_no_remove_these_sw]\n",
    "\n",
    "\n",
    "def stem_words(from_text):\n",
    "    stemmer = SnowballStemmer(\"romanian\")\n",
    "    stemmer2 = SnowballStemmer(\"romanian\", ignore_stopwords=True)\n",
    "    return [\" \".join([stemmer.stem(word) for word in sentence.split(\" \")]) for sentence in from_text]\n",
    "\n",
    "\n",
    "def lemmatize_words(sentence):\n",
    "    from pywsd.utils import lemmatize_sentence\n",
    "    return lemmatize_sentence(sentence)\n",
    "\n",
    "\n",
    "def sentence_tokenization(sentence):\n",
    "    return word_tokenize(sentence)\n",
    "\n",
    "\n",
    "def sentence_punct_tokenization(sentence):\n",
    "    return WordPunctTokenizer().tokenize(sentence)\n",
    "\n",
    "\n",
    "def sentence_split_tokenization(sentence):\n",
    "    return ([i for i in re.split(' ', sentence) if i])\n",
    "\n",
    "\n",
    "def remove_apostrophe_words(train):\n",
    "    train = [w.replace(\"it's\", 'it is')\n",
    "                     .replace(\"that's\", \"that is\")\n",
    "                     .replace(\"it 's\", 'it is')\n",
    "                     .replace(\"that 's\", \"that is\")\n",
    "                     .replace(\"'ve\", \" have\")\n",
    "                     .replace(\"' ve\", \" have\")\n",
    "                     .replace(\"won't\", \"will not\")\n",
    "                     .replace(\"wo n't\", \"will not\")\n",
    "                     .replace(\"don't\", \"do not\")\n",
    "                     .replace(\"do n't\", \"do not\")\n",
    "                     .replace(\"can't\", \"can not\")\n",
    "                     .replace(\"ca n't\", \"can not\")\n",
    "                     .replace(\"sha n't\", \"shall not\")\n",
    "                     .replace(\"shan't\", \"shall not\")\n",
    "                     .replace(\"n't\", \" not\")\n",
    "                     .replace(\"'re\", \" are\")\n",
    "                     .replace(\"'d\", \" would\")\n",
    "                     .replace(\"'ll\", \" will\") for w in train]\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_sentences(X, y):\n",
    "    new_X = []\n",
    "    new_y = []\n",
    "    for i in range(len(X)):\n",
    "        if len(X[i].split()) > 1:\n",
    "            new_X.append(X[i])\n",
    "            new_y.append(y[i])\n",
    "    return new_X, new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(X_train):\n",
    "    stopwords = set(nltk.corpus.stopwords.words('romanian'))\n",
    "    \n",
    "    tokenized_sentence = []\n",
    "    for sentence in X_train:\n",
    "        tokenized_sentence.append(sentence_punct_tokenization(sentence))\n",
    "\n",
    "    # NO STOP WORDS\n",
    "    train_without_stopwords = remove_stopwords(tokenized_sentence, stopwords)\n",
    "\n",
    "    # NO PUNCTUATION\n",
    "    train_without_punctuation = remove_punctuation(train_without_stopwords)\n",
    "    train_clean_spaces = remove_spaces(train_without_punctuation)\n",
    "\n",
    "    # STEM WORDS\n",
    "#     stemmed_train = stem_words(X_train)\n",
    "\n",
    "    # LEMM WORDS\n",
    "#     lemmatized_train = ([lemmatize_words(sentence) for sentence in X_train])\n",
    "\n",
    "    # APPLY LITTLE FEATURES\n",
    "    # feat_train = remove_stopwords(tokenized_sentence, updated_stopwords)\n",
    "    # feat_train = remove_punctuation(feat_train)\n",
    "    # feat_train = stem_words(feat_train)\n",
    "    # feat_train = ([lemmatize_words(sentence) for sentence in feat_train])\n",
    "\n",
    "    # return based on what features Ii want to extract\n",
    "    # there are 5 possible ways of returning: extract stop words, extract punctuation,\n",
    "    # word stem, word lemm, and all the features\n",
    "    return train_clean_spaces # we choose to extract stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the text data.\n",
    "newsgroups_train = fetch_20newsgroups(subset = 'train', shuffle = True)\n",
    "newsgroups_test = fetch_20newsgroups(subset = 'test', shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data pos len: 11077\n",
      "data neg len: 23509\n"
     ]
    }
   ],
   "source": [
    "train_pos, train_neg, test_pos, test_neg = load_romanian_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/lemma/train_pos_lemma.txt\") as pos_lemma:\n",
    "    train_pos_lemma = pos_lemma.readlines()\n",
    "with open(data/lemma/\"train_neg_lemma.txt\") as neg_lemma:\n",
    "    train_neg_lemma = neg_lemma.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = prepare_train_and_test(train_pos_lemma, train_neg_lemma, test_pos, test_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_train = feature(X_train)\n",
    "# feat_train, y_train = remove_empty_sentences(feat_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PCA does not support sparse input. See TruncatedSVD for a possible alternative.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a6bd2de393a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mx_train_pca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mx_test_pca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/pca.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \"\"\"\n\u001b[0;32m--> 360\u001b[0;31m         \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m         \u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_components_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/pca.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;31m# This is more informative than the generic one raised by check_array.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m             raise TypeError('PCA does not support sparse input. See '\n\u001b[0m\u001b[1;32m    379\u001b[0m                             'TruncatedSVD for a possible alternative.')\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: PCA does not support sparse input. See TruncatedSVD for a possible alternative."
     ]
    }
   ],
   "source": [
    "# Feature extraction (text vectorization) using term frequency - inverse document frequency.\n",
    "tfidf_vect = TfidfVectorizer(ngram_range = (1,3))\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "X_train = remove_punctuation(X_train)\n",
    "X_train = remove_spaces(X_train)\n",
    "\n",
    "x_train = tfidf_vect.fit_transform(X_train)\n",
    "x_test = tfidf_vect.transform(X_test)\n",
    "\n",
    "# x_train_pca = pca.fit_transform(x_train)\n",
    "# x_test_pca = pca.transform(x_test)\n",
    "\n",
    "\n",
    "print('Number of training data is ' + str(x_train.shape[0]))\n",
    "print('Number of test data is ' + str(x_test.shape[0]))\n",
    "print('Data dimension is ' + str(x_train.shape[1]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=4\n",
    "# print(str(y_train[idx]) + \"\\t\" + feat_train[idx])\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Multinomial Naive Bayes.\n",
    "NB_clf = MultinomialNB(alpha = 0.03).fit(x_train, y_train)\n",
    "predicted_test = NB_clf.predict(x_test)\n",
    "predicted_train = NB_clf.predict(x_train)\n",
    "print('========== 1. Multinomial Naive Bayes ==========')\n",
    "print('The F-1 score for test query is ' + str(metrics.f1_score(y_test, predicted_test, average = 'macro')))\n",
    "print('Training accuracy of naive bayes model is ' + str(np.mean(predicted_train == y_train)))\n",
    "print('Test accuracy of naive bayes model is ' + str(np.mean(predicted_test == y_test)))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. K-Nearest-Neighbors.\n",
    "knn_clf = KNeighborsClassifier(n_neighbors = 2000)\n",
    "knn_clf.fit(x_train, y_train)\n",
    "predicted_test = knn_clf.predict(x_test)\n",
    "predicted_train = knn_clf.predict(x_train)\n",
    "print('========== 2. K-Nearest-Neighbors ==========')\n",
    "print('The F-1 score for test query is ' + str(metrics.f1_score(y_test, predicted_test, average = 'macro')))\n",
    "print('Training accuracy of KNN model is ' + str(np.mean(predicted_train == y_train)))\n",
    "print('Test accuracy of KNN model is ' + str(np.mean(predicted_test == y_test)))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Random Forest.\n",
    "rf_clf = RandomForestClassifier(n_estimators = 200, max_depth = 100)\n",
    "rf_clf.fit(x_train, y_train)\n",
    "predicted_test = rf_clf.predict(x_test)\n",
    "predicted_train = rf_clf.predict(x_train)\n",
    "print('========== 3. Random Forest ==========')\n",
    "print('The F-1 score for test query is ' + str(metrics.f1_score(y_test, predicted_test, average = 'macro')))\n",
    "print('Training accuracy of random forest model is ' + str(np.mean(predicted_train == y_train)))\n",
    "print('Test accuracy of random forest model is ' + str(np.mean(predicted_test == y_test)))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Linear Support Vector Machine\n",
    "svm_clf = LinearSVC(loss = 'hinge', penalty = 'l2', tol = 1e-4, max_iter = 1000)\n",
    "svm_clf.fit(x_train, y_train)\n",
    "predicted_test = svm_clf.predict(x_test)\n",
    "predicted_train = svm_clf.predict(x_train)\n",
    "print('========== 4. Support Vector Machine with Linear Kernel ==========')\n",
    "print('The F-1 score for test query is ' + str(metrics.f1_score(y_test, predicted_test, average = 'macro')))\n",
    "print('Training accuracy of SVM model is ' + str(np.mean(predicted_train == y_train)))\n",
    "print('Test accuracy of SVM model is ' + str(np.mean(predicted_test == y_test)))\n",
    "print('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
